{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSFSfQrhpBIy"
      },
      "source": [
        "# **Feature Selection**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFCTwQXWpFVD"
      },
      "source": [
        "## **Agenda**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGUyaIQPpNGI"
      },
      "source": [
        "In this lesson, we will cover the following concepts with the help of a business use case:\n",
        "* Feature Selection\n",
        "* Dimensionality Reduction:\n",
        "  * Dimensionality Reduction Techniques\n",
        "  * Pros and Cons of Dimensionality Reduction\n",
        "  * Factor Analysis\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmob5c_rLq7j"
      },
      "source": [
        "## **What Is Feature Selection?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VA2XJ5h1Lynm"
      },
      "source": [
        "Feature selection is a method that helps in the inclusion of the significant variables that<br> help form a model with good predictive power.\n",
        "\n",
        "Features or variables that are redundant or irrelevant can negatively impact the<br> performance of the model, thus it becomes necessary to remove them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Sr_qFfvbP2y"
      },
      "source": [
        "### **Benefits of Feature Selection**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_0Mn4FKdRsm"
      },
      "source": [
        "* It reduces overfitting as the unwanted variables are removed, and the focus is on the significant variables.\n",
        "\n",
        "* It removes irrelevant information, which helps to improve the accuracy of the model’s predictions.\n",
        "\n",
        "* It reduces the computation time involved to get the model's predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrWVaY81T5Sc"
      },
      "source": [
        "## **Dimensionality Reduction**\n",
        "\n",
        "Dimensionality reduction is the method of transforming a collection of data having large dimensions into data of smaller dimensions while ensuring that identical information is conveyed concisely.\n",
        "\n",
        "### **Dimensionality Reduction Techniques**\n",
        "\n",
        "Some of the techniques used for dimensionality reduction are:\n",
        "\n",
        "1. Imputing missing values\n",
        "2. Dropping low-variance variables\n",
        "3. Decision trees (DT)\n",
        "4. Random forest (RF)\n",
        "5. Reducing highly correlated variables\n",
        "6. Backward feature elimination\n",
        "7. Factor analysis\n",
        "\n",
        "\n",
        "### **Pros of Dimensionality Reduction**\n",
        "\n",
        "- It helps to compress data, reducing the storage space needed.\n",
        "- It cuts down on computing time.\n",
        "- It also aids in the removal of redundant features.\n",
        "\n",
        "### **Cons of Dimensionality Reduction**\n",
        "\n",
        "- Some data may will be lost as a result.\n",
        "- We use certain thumb rules when we do not know how many principal components to keep in practice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxAtmkqzsY9-"
      },
      "source": [
        "## **Gist of Factor Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omeHf4JJsazA"
      },
      "source": [
        "* Factor analysis is used to:\n",
        "  * Explain variance among the observed variables\n",
        "  * Condense the set of observed variables into the factors\n",
        "\n",
        "  ![FA](https://labcontent.simplicdn.net/data-content/content-assets/Data_and_AI/Applied_Machine_Learning/Images/0.6_Feature_Selection/Trainer_PPT_and_IPYNB/FA.JPG)\n",
        "\n",
        "* Factor explains the amount of variance in the observed variables.\n",
        "\n",
        "* In other words, factor analysis is a method that investigates linear relation of a number of variables of interest V1, V2,……., Vl, with a smaller number of unobservable factors F1, F2,..……, Fk.\n",
        "<br><br>\n",
        "  ![FA1](https://labcontent.simplicdn.net/data-content/content-assets/Data_and_AI/Applied_Machine_Learning/Images/0.6_Feature_Selection/Trainer_PPT_and_IPYNB/FA1.JPG)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PY30SMvIuOaU"
      },
      "source": [
        "### **Types of FA**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzP9afLCuQ0r"
      },
      "source": [
        "![FA2](https://labcontent.simplicdn.net/data-content/content-assets/Data_and_AI/Applied_Machine_Learning/Images/0.6_Feature_Selection/Trainer_PPT_and_IPYNB/FA2.JPG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "neVrajUnu-Gk"
      },
      "source": [
        "### **Work Process of FA**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHgsVtDWyJIw"
      },
      "source": [
        "The objective of the factor analysis is the reduction of the number of observed variables and find the unobservable variables.\n",
        "\n",
        "It is a two-step process.\n",
        "\n",
        "![FA3](https://labcontent.simplicdn.net/data-content/content-assets/Data_and_AI/Applied_Machine_Learning/Images/0.6_Feature_Selection/Trainer_PPT_and_IPYNB/FA3.JPG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WznRxFwz0xT0"
      },
      "source": [
        "### **Choosing Factors**\n",
        "\n",
        "Note: Let us get accustomed to the term eigenvalue before moving to the selecting the number of factors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1ofi7fP0zC8"
      },
      "source": [
        "**Eigenvalues:**\n",
        "\n",
        "It represents the explained variance of each factor from the total variance and is also known as the characteristic roots."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlYneF_q02Em"
      },
      "source": [
        "**Ways to Choose Factors:**\n",
        "\n",
        "* The eigenvalues are a good measure for identifying the significant factors. An eigenvalue greater than 1 is considered for the selection criteria of the feature.\n",
        "\n",
        "* Apart from observing values, the graphical approach is used that visually represents the factors' eigenvalues. This visualization is known as the scree plot. A Scree plot helps in determining the number of factors where the curve makes an elbow.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWLSxO2lce6l"
      },
      "source": [
        "### **Use Case: Feature Selection in Cancer Dataset Using FA**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAdVv1cKce6n"
      },
      "source": [
        "**Problem Statement**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZJFc7mece6o"
      },
      "source": [
        "John Cancer Hospital (JCH) is a leading cancer hospital in the USA. It specializes in preventing breast cancer.\n",
        "\n",
        "Over the last few years, JCH has collected breast cancer data from patients who came for screening or treatment.\n",
        "\n",
        "However, this data has 32 attributes and is difficult to run and interpret the result. As an ML expert,\n",
        "\n",
        " you have to reduce the number of attributes so that the results are meaningful and accurate.\n",
        "\n",
        "Use FA for feature selection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Pf2xDB6ce6o"
      },
      "source": [
        "#### **Dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRcJ5Puvce6p"
      },
      "source": [
        "Features of the dataset are computed from a digitized image of a Fine-Needle Aspirate (FNA) of a breast mass.\n",
        "\n",
        "They describe the characteristics of the cell nuclei present in the image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTFnzjDRce6q"
      },
      "source": [
        "#### **Data Dictionary**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gL5Y14LZce6r"
      },
      "source": [
        "**Dimensions:**\n",
        "* 32 variables\n",
        "* 569 observations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-_tp9Ypce6r"
      },
      "source": [
        "**Attribute Information:**\n",
        "\n",
        "1. ID number\n",
        "\n",
        "2. Diagnosis (M = malignant, B = benign)\n",
        "\n",
        "3. Attributes with mean values: <br>\n",
        "10 real-valued features are computed for each cell nucleus:\n",
        "  * radius_mean (mean of distances from center to points on the perimeter)\n",
        "  * texture_mean (standard deviation of gray-scale values)\n",
        "  * perimeter_mean\n",
        "  * area_mean\n",
        "  * smoothness_mean (local variation in radius lengths)\n",
        "  * compactness_mean (perimeter$^2$ / area - 1.0)\n",
        "  * concavity_mean (severity of concave portions of the contour)\n",
        "  * concave points_mean (number of concave portions of the contour)\n",
        "  * symmetry_mean\n",
        "  * fractal dimension_mean (\"coastline approximation\" - 1)\n",
        "\n",
        "4. Attributes with standard error and worst/largest:\n",
        "  * radius_se\n",
        "  * texture_se\n",
        "  * perimeter_se\n",
        "  * area_se\n",
        "  * smoothness_se\n",
        "  * compactness_se\n",
        "  * concavity_se\n",
        "  * concave points_se\n",
        "  * symmetry_se\n",
        "  * fractal_dimension_se\n",
        "  * radius_worst\n",
        "  * texture_worst\n",
        "  * perimeter_worst\n",
        "  * area_worst\n",
        "  * smoothness_worst\n",
        "  * compactness_worst\n",
        "  * concavity_worst\n",
        "  * concave points_worst\n",
        "  * symmetry_worst\n",
        "  * fractal_dimension_worst\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNL6JVUEce6s"
      },
      "source": [
        "#### **Solution**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDogpt2Sce6s"
      },
      "source": [
        "##### **Import Libraries**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KVcSOGBabQH"
      },
      "source": [
        "In Python, Numpy is a package that includes multidimensional array objects as well as a number of derived objects.\n",
        "Matplotlib is an amazing visualization library in Python for 2D plots of arrays.\\n\n",
        "Pandas is used for data manipulation and analysis\\n\n",
        "So these are the core libraries that are used for the EDA process.\n",
        "\n",
        "These libraries are written with an import keyword."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CP_NZx7gce6s"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9zbLtunce6v"
      },
      "source": [
        "##### **Import and Check the Data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qNQpYxFabQJ"
      },
      "source": [
        "Before reading data from a csv file, you need to download the \"breast-cancer-data.csv\" dataset from the resource section and upload it into the Lab.\n",
        "We will use the Up arrow icon, which is shown on the left side under the View icon. Click on the Up arrow icon and upload the file from\n",
        "wherever it has downloaded into your system.\n",
        "\n",
        "After this, you will see the downloaded file will be visible on the left side of your lab along with all the .pynb files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_lVXiKc6ce6w"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('breast-cancer-data.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrscV9c7abQJ"
      },
      "source": [
        "pd.read_csv function is used to read the \"breast-cancer-data.csv\" file and df.head() will show the top 5 rows of the dataset.\n",
        "\n",
        "* dataframe or df is a variable that will store the data read by the csv file.\n",
        "* head will show the rows and () default take the 5 top rows as output.\n",
        "* one more example - df.head(3) will show the top 3 rows."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UMyk59-abQJ"
      },
      "source": [
        "#### shape function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tM3Y8scBce6z"
      },
      "outputs": [],
      "source": [
        "\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVUQa7FEabQK"
      },
      "source": [
        "df.shape will show the number of rows and columns in the dataframe."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQf4orHrabQK"
      },
      "source": [
        "#### info Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1yLNdq2Sce61"
      },
      "outputs": [],
      "source": [
        "# Check the data , there should be no missing values\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-_Vad0wabQK"
      },
      "source": [
        "* The dataframe's information is printed using the info() function.\n",
        "* The number of columns, column labels, column data types, memory use, range index, and the number of cells in each column are all included in the data (non-null values)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IlRPCvp2ce62"
      },
      "outputs": [],
      "source": [
        "\n",
        "# defining the array as np.array\n",
        "feature_names = np.array(['mean radius' 'mean texture' 'mean perimeter' 'mean area'\n",
        " 'mean smoothness' 'mean compactness' 'mean concavity'\n",
        " 'mean concave points' 'mean symmetry' 'mean fractal dimension'\n",
        " 'radius error' 'texture error' 'perimeter error' 'area error'\n",
        " 'smoothness error' 'compactness error' 'concavity error'\n",
        " 'concave points error' 'symmetry error' 'fractal dimension error'\n",
        " 'worst radius' 'worst texture' 'worst perimeter' 'worst area'\n",
        " 'worst smoothness' 'worst compactness' 'worst concavity'\n",
        " 'worst concave points' 'worst symmetry' 'worst fractal dimension'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IQVSwn2Xce64"
      },
      "outputs": [],
      "source": [
        "#### Convert diagnosis column to 1/0 and store in new column target\n",
        "from sklearn.preprocessing import LabelEncoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2evEvFaUabQL"
      },
      "source": [
        "\n",
        "* The sklearn.preprocessing package contains a number of useful utility methods and transformer classes for converting raw feature vectors into a format that is suitable for downstream estimators.\n",
        "* LabelEncoder encodes labels with a value between 0 and n_classes-1 where n is the number of distinct labels.\n",
        "* These libraries are written with an import keyword."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mi_rj_Yhce66"
      },
      "outputs": [],
      "source": [
        "# # Encode label diagnosis\n",
        "#M -> 1\n",
        "#B -> 0\n",
        "\n",
        "#Converting diagnosis to numerical variable in df\n",
        "df['diagnosis'] = df['diagnosis'].map({'M': 1, 'B': 0}).astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCYMBTQ4abQL"
      },
      "source": [
        "In the above code, we are encoding the column diagnosis in which we are encoding M as 1 and B as 0."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXqK7XiNabQM"
      },
      "source": [
        "## Factor Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fb0AXBpQabQM"
      },
      "source": [
        "* A linear statistical model is a factor analysis.\n",
        "* It is used to condense a group of observable variables into an unobserved variable termed factors and to explain the variance among the observed variables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRvKiS6_ce7A"
      },
      "source": [
        "#### **Adequacy Test**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8dFOAxaabQM"
      },
      "source": [
        "Before you perform factor analysis, you need to evaluate the “factorability” of our dataset. Can we find the factors in the dataset? Checking factorability or sampling adequacy can be done in two ways:\n",
        "\n",
        "1- The Bartlett's Test\n",
        "\n",
        "2- Test of Kaiser-Meyer-Olkin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9S0FafZhzej"
      },
      "outputs": [],
      "source": [
        "#Install factor analyzer\n",
        "#!pip install factor_analyzer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7Ns2vbyjVtB"
      },
      "source": [
        "\n",
        "#### **Bartlett's Test**\n",
        "\n",
        "Bartlett’s test of sphericity checks whether or not the observed variables intercorrelate at all using the observed correlation matrix against the identity matrix. If the test is found to be statistically insignificant, you should not employ a factor analysis.\n",
        "\n",
        "Note: This test checks for the intercorrelation of observed variables by comparing the observed correlation matrix against the identity matrix.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kz3-9N9cabQN"
      },
      "outputs": [],
      "source": [
        "!pip install factor_analyzer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59jRTLxLabQN"
      },
      "source": [
        "* In the above code, we are installing the factor analyzer.\n",
        "* pip is used to install the packages.\n",
        "* Factor analysis is an exploratory data analysis method used to search for influential underlying factors or latent variables from a set of observed variables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9ZXXVQuabQN"
      },
      "source": [
        "Now, you are trying to perform factor analysis by using the factor analyzer module. Use the below code\n",
        "for calculating_bartlett_sphericity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QaWYEa4Ifint"
      },
      "outputs": [],
      "source": [
        "from factor_analyzer import FactorAnalyzer\n",
        "from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity\n",
        "chi_square_value,p_value=calculate_bartlett_sphericity(df)\n",
        "chi_square_value, p_value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46v4RI5PabQN"
      },
      "source": [
        "* In the above code, you are importing the factor analyzer and calculate_bartlett_sphericity.\n",
        "* In this Bartlett’s test, the p-value is 0. The test was statistically significant, indicating that the observed correlation matrix is not an identity matrix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bfr2qW3ZjRAc"
      },
      "source": [
        "**Inference:**\n",
        "\n",
        "The p-value is 0, and this indicates that the test is statistically significant and highlights that the correlation matrix is not an identity matrix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArAG3cZEjlkn"
      },
      "source": [
        "#### **Kaiser-Meyer-Olkin Test**\n",
        "\n",
        "* The Kaiser-Meyer-Olkin (KMO) test determines if data is suitable for factor analysis.\n",
        "* It assesses the suitability of each observed variable as well as the entire model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXMWtlgkce7A"
      },
      "outputs": [],
      "source": [
        "from factor_analyzer.factor_analyzer import calculate_kmo\n",
        "kmo_all,kmo_model=calculate_kmo(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HPTWaF7MmN07"
      },
      "outputs": [],
      "source": [
        "kmo_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYgJghqCabQO"
      },
      "source": [
        "* In the above code, we are calculating the KMO. KMO estimates the proportion of variance among all the observed variables.\n",
        "* The overall KMO for the data is 0.25, which is excellent. This value indicates that you can proceed with your planned factor analysis.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u40ACrK6oIHY"
      },
      "outputs": [],
      "source": [
        "df.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xE-3rJYCce7C"
      },
      "source": [
        "**Inference:**\n",
        "\n",
        "The KMO value is less than 0.5, and this indicates that we need to delete the insignificant variables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5ARN-6mqKPF"
      },
      "source": [
        "**Finding Significant Variables**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5EAqKzACce7C"
      },
      "outputs": [],
      "source": [
        "corr = df.corr()\n",
        "corr.style.background_gradient(cmap='coolwarm')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wntI4Lo_abQP"
      },
      "source": [
        "\n",
        "\n",
        "If your main goal is to visualize the correlation matrix rather than creating a plot per se, the convenient pandas styling options are a viable built-in solution, as shown in the above code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0u1-Jx_9sF9B"
      },
      "source": [
        "**Creating Dataset of Significant Variables**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3dEgIDudrOxX"
      },
      "outputs": [],
      "source": [
        "df_corr = df[['radius_mean','perimeter_mean', 'area_mean','radius_worst','perimeter_worst',\n",
        "              'area_worst','concavity_mean','concave points_mean', 'concavity_worst',\n",
        "              'concave points_worst','diagnosis']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UCx9ELIsUK-"
      },
      "source": [
        "**Running KMO Test**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8cJuObwfr20a"
      },
      "outputs": [],
      "source": [
        "from factor_analyzer.factor_analyzer import calculate_kmo\n",
        "kmo_all,kmo_model=calculate_kmo(df_corr)\n",
        "kmo_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2eAjgSynabQQ"
      },
      "source": [
        "* In the above code, we are calculating the KMO. KMO estimates the proportion of variance among all the observed variables.\n",
        "* The overall KMO for the data is 0.82, which is excellent. This value indicates that you can proceed with your planned factor analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1za8nwW3jbB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "meiYWjRdce7R"
      },
      "source": [
        "**Inference:**\n",
        "\n",
        "The value of the KMO model is greater than 0.5. Therefore, the dataset is good enough for factor analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8M3sIztt30d"
      },
      "source": [
        "##### **Factor Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kx9sn4PBuA6W"
      },
      "outputs": [],
      "source": [
        "from factor_analyzer import FactorAnalyzer\n",
        "fa = FactorAnalyzer(rotation='varimax')\n",
        "fa.fit(df_corr,11)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0NCc4BlabQS"
      },
      "source": [
        "In the above code, we create a factor analysis object and perform factor analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IUU8fESxwZFe"
      },
      "outputs": [],
      "source": [
        "ev, v = fa.get_eigenvalues()\n",
        "ev"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "symxLmsnabQS"
      },
      "source": [
        "* Here, you can see only for 2-factors eigenvalues are greater than one. It means we only need to choose 2 factors (or unobserved variables).\n",
        "* In the above code, we are checking the Eigenvalues.\n",
        "* It measures how much of the variance of the variables a factor explains.\n",
        "* An eigenvalue of more than one means that the factor explains more variance than a unique variable.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kCmO6m1_1ERg"
      },
      "outputs": [],
      "source": [
        "plt.scatter(range(1,df_corr.shape[1]+1),ev)\n",
        "plt.plot(range(1,df_corr.shape[1]+1),ev)\n",
        "plt.title('Scree Plot')\n",
        "plt.xlabel('Factors')\n",
        "plt.ylabel('Eigenvalue')\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2k3xYUSabQS"
      },
      "source": [
        "In the above code, we are creating screen plot using matplotlib module."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJ2zQFbhxcL7"
      },
      "source": [
        "**Inference:**\n",
        "\n",
        "* Referring eigenvalues: There are only two values above 1. Therefore, we will choose only 2 factors (or unobserved variables).\n",
        "\n",
        "* Referring Scree plot: There are only two values after the elbow. Therefore, we will choose only 2 factors (or unobserved variables)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQ_ocrSO_s-o"
      },
      "outputs": [],
      "source": [
        "fa1 = FactorAnalyzer(rotation=\"varimax\", n_factors=2)\n",
        "fa1.fit(df, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fa5hpF___bJ"
      },
      "outputs": [],
      "source": [
        "fa1.loadings_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odbmkgjZ0AUU"
      },
      "source": [
        "##### **Confirmatory Factor Analysis**\n",
        "\n",
        "* It determines the factors and factor loadings of measured variables.\n",
        "* The general assumption of CFA is that every factor has an association with a specific subset of the measured variables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-FbTsrMqLQ9"
      },
      "source": [
        "##### A ConfirmatoryFactorAnalyzer class, which fits a confirmatory factor analysis model using maximum likelihood.\n",
        "\n",
        "Parameters:\n",
        "\n",
        "*  specification (ModelSpecificaition object or None, optional): It is a model specification that must be a ModelSpecificaiton object or None. If None, the ModelSpecification will be generated assuming that n_factors == n_variables, and that all variables load on all factors.\n",
        "\n",
        "  Note that this could mean the factor model is not identified, and the optimization could fail. Hence, the factor value will be taken as none by default.\n",
        "\n",
        "\n",
        "* n_obs (int or None, optional): It indicates the number of observations in the original data set. If this is not passed and is_cov_matrix=True, then an error will be raised. Hence, the value of n_obs will be default to None.\n",
        "\n",
        "* is_cov_matrix (bool, optional): It denotes whether the input X is a covariance matrix. If False, assume it is the full data set. Defaults to False.\n",
        "\n",
        "* bounds (list of tuples or None, optional): It is a list of minimum and maximum boundaries for each element of the input array.\n",
        "This must equal x0, which is the input array from your parsed and combined model specification. The length is:\n",
        "\n",
        "((n_factors * n_variables) + n_variables + n_factors + (((n_factors * n_factors) - n_factors) // 2)\n",
        "\n",
        " If None, nothing will be bounded. Defaults to None.\n",
        "\n",
        "* max_iter (int, optional): It is the maximum number of iterations for the optimization routine. Defaults to 200.\n",
        "\n",
        "* tol (float or None, optional): It is the tolerance for convergence. Defaults to None.\n",
        "\n",
        "* disp (bool, optional): It denotes whether to print the scipy optimization fmin message to standard output. Defaults to True.\n",
        "\n",
        "Raises:\tValueError – If is_cov_matrix is True, and n_obs is not provided.\n",
        "\n",
        "model\n",
        "\n",
        "    The model specification object.\n",
        "    Type: ModelSpecification\n",
        "\n",
        "loadings_\n",
        "\n",
        "    The factor loadings matrix.\n",
        "    Type: numpy array\n",
        "\n",
        "error_vars_\n",
        "\n",
        "    The error variance matrix\n",
        "    Type: numpy array\n",
        "\n",
        "factor_varcovs_\n",
        "\n",
        "    The factor covariance matrix.\n",
        "    Type: numpy array\n",
        "\n",
        "log_likelihood_\n",
        "\n",
        "    The log likelihood from the optimization routine.\n",
        "    Type: float"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bMTDfU4vqLQ9"
      },
      "outputs": [],
      "source": [
        "#  CFA stands for Confirmatory Factor Analysis.This line imports CFA function\n",
        "# from factor_analyzer import (ConfirmatoryFactorAnalyzer, ModelSpecificationParser)\n",
        "from factor_analyzer import (ConfirmatoryFactorAnalyzer, ModelSpecificationParser)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WstnD140D54"
      },
      "source": [
        "\n",
        "\n",
        "Here, 'radius_mean', 'perimeter_mean', 'area_mean', 'radius_worst', 'perimeter_worst', 'diagnosis', 'area_worst', 'concavity_mean',\n",
        " 'concave points_mean', 'concavity_worst', 'concave points_worst' refers to the name of columns in the data frame\n",
        " that you'd like to allocate to each factor (F1 and F2).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mgmLajnnqLQ-"
      },
      "outputs": [],
      "source": [
        "model_dict = {\"F1\": ['radius_mean','perimeter_mean', 'area_mean','radius_worst',\n",
        "                     'perimeter_worst','diagnosis'], \"F2\": ['area_worst','concavity_mean',\n",
        "                    'concave points_mean', 'concavity_worst','concave points_worst']}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u14_JZkIqLQ-"
      },
      "source": [
        "##### class ModelSpecificationParser:\n",
        "\n",
        "    A class to generate the model specification for CFA.\n",
        "    This class includes two static methods to generate the\n",
        "    \"ModelSpecification\" object from either a dictionary\n",
        "    or a numpy array.\n",
        "\n",
        "###### def parse_model_specification_from_dict(X, specification=None):\n",
        "\n",
        "        Generate the model specification from a\n",
        "        dictionary. The keys in the dictionary\n",
        "        should be the factor names, and the values\n",
        "        should be the feature names. If this method\n",
        "        is used to create the \"ModelSpecification\",\n",
        "        then factor names and variable names will\n",
        "        be added as properties to that object.\n",
        "\n",
        "##### Parameters\n",
        "        ----------\n",
        "        X : array-like\n",
        "            The data set that will be used for CFA.\n",
        "        specification : dict or None\n",
        "            A dictionary with the loading details. If None, the matrix will\n",
        "            be created assuming all variables load on all factors.\n",
        "            Defaults to None.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        ModelSpecification: A model specification object\n",
        "\n",
        "        Raises\n",
        "        ------\n",
        "        ValueError: If 'specification' is not in the expected format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eqpaGf-JqLQ-"
      },
      "outputs": [],
      "source": [
        "\n",
        "model_spec = ModelSpecificationParser.parse_model_specification_from_dict(df_corr,model_dict)\n",
        "# df_corr:Pandas DataFrame and model_dict: dictionary with factors\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJy_BI3wqLQ-"
      },
      "source": [
        "    def fit(self, X, y=None):\n",
        "        \"\"\"\n",
        "        Perform confirmatory factor analysis.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : array-like\n",
        "            The data to use for confirmatory\n",
        "            factor analysis. If this is just a\n",
        "            covariance matrix, make sure 'is_cov_matrix'\n",
        "            was set to True.\n",
        "        y : ignored\n",
        "\n",
        "        Raises\n",
        "        ------\n",
        "        ValueError:If the specification is not None or a \"ModelSpecification\" object\n",
        "        AssertionError:If \"is_cov_matrix=True\" and the matrix is not square.\n",
        "        AssertionError:If len(bounds) != len(x0)\n",
        "        \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYXzm60uqLQ-"
      },
      "outputs": [],
      "source": [
        "# Performs confirmatory factor analysis\n",
        "cfa1 = ConfirmatoryFactorAnalyzer(model_spec, disp=False)\n",
        "cfa1.fit(df_corr.values)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gAVyEr99qLQ-"
      },
      "outputs": [],
      "source": [
        "# cfa1.loadings_ will gave you the factor loading matrix\n",
        "# The factor loading is a matrix which shows the relationship of each variable to the underlying factor.\n",
        "# It shows the correlation coefficient for observed variable and factor.\n",
        "# It shows the variance explained by the observed variables.\n",
        "cfa1.loadings_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ufmAJKVR41_j"
      },
      "outputs": [],
      "source": [
        "#This will give you the factor covariance matrix and the type of this will be numpy array\n",
        "cfa1.factor_varcovs_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6QR3ubLrqLQ_"
      },
      "outputs": [],
      "source": [
        "# transform(X) used to get the factor scores for new data set.\n",
        "\n",
        "# Parameters:X (array-like, shape (n_samples, n_features)) – The data to score using the fitted factor model.\n",
        "# Returns: scores – The latent variables of X.\n",
        "# Return type:numpy array, shape (n_samples, n_components)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hpEes4kw44aP"
      },
      "outputs": [],
      "source": [
        "cfa1.transform(df_corr.values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mz1c3ds3sav"
      },
      "source": [
        "**Note: In this lesson, we saw the use of the feature selection methods, but in the next lesson we are going to use\n",
        "    one of these methods as a sub-component of \"Supervised Learning - Regression and Classification\".**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sd-UtEKgZl5Q"
      },
      "source": [
        "![Simplilearn_Logo](https://labcontent.simplicdn.net/data-content/content-assets/Data_and_AI/Logo_Powered_By_Simplilearn/SL_Logo_1.png)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}